name: "HRL Configuration"

# This configuration serves the purpose of documenting and explaining the settings, *NOT* as an example for good hyperparamter settings.

data: # specify your data here
    env_name: HRLEnvironment
    scenario: "HRL"
    max_questions_per_subtask: 5     # maximum number of clarification questions per subtask
    beta: 0.3                        # Absolute value of reward given for negative answer
    reward_for_correct: 1            # Reward given for correct prediction
    reward_for_incorrect: -1         # Reward given for incorrect prediction
    num_subtasks: 4                  # For calculating accuracy. This is not a setter.

training: # specify training details here
   #load_agent: "best.ckpt"          # if given, load a pre-trained model from this checkpoint
    warmstart: True                  # if True, load the agent checkpoint specified in `load_agent` above
    reset_best_ckpt: False           # if True, reset the tracking of the best checkpoint and scores. Use for domain adaptation or fine-tuning with new metrics or dev data.
    reset_scheduler: False           # if True, overwrite scheduler in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.
    reset_optimizer: False           # if True, overwrite optimizer in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.
    random_seed: 42                  # set this seed to make training deterministic
    optimizer: "sgd"                 # choices: "sgd", "adam", "adadelta", "adagrad", "rmsprop", default is SGD
    adam_betas: [0.9, 0.999]         # beta parameters for Adam. These are the defaults. Typically these are different for Transformer models.
    learning_rate: 0.001             # initial learning rate, default: 3.0e-4
    learning_rate_min: 0.0001        # stop learning when learning rate is reduced below this threshold, default: 1.0e-8
    clip_grad_val: 1.0               # clip the gradients to this value when they exceed it, optional
    weight_decay: 0.                 # l2 regularization, default: 0
    scheduling: "plateau"            # learning rate scheduling, optional, if not specified stays constant, options: "plateau", "exponential", "decaying", "noam" (for Transformer), "warmupexponentialdecay"
    patience: 5                      # specific to plateau scheduler: wait for this many validations without improvement before decreasing the learning rate
    decrease_factor: 0.5             # specific to plateau & exponential scheduler: decrease the learning rate by this factor
    shuffle: True                    # shuffle the training data, default: True
    use_cuda: True                   # use CUDA for acceleration on GPU, required. Set to False when working on CPU.
    keep_last_ckpts: 3               # keep this many of the latest checkpoints, if -1: all of them, default: 5
    overwrite: False                 # overwrite existing model directory, default: False. Do not set to True unless for debugging!
    epochs: 12                       # train for this many epochs
    episodes_per_epoch: 233076       # episodes per epoch
    max_steps_per_episode: 250       # maximum trajectory length per episode
    update_every: 64                 # update agents & value networks after this many episodes
    update_target_every: 256         # update target networks after this many value net updates
    validation_freq: 10000           # validate after this many episodes (default: 1000)
    num_valid_episodes: 6000         # run this many episodes for evaluation
    logging_freq: 100                # log the training progress after this many updates, default: 100
    eval_metric: "reward"            # validation metric, default: "reward", other options: "loss"
    early_stopping_metric: "reward"  # when a new high score on this metric is achieved, a checkpoint is written, when "eval_metric" (default) is maximized, when "loss" or "ppl" is minimized
    model_dir: "models/hrl_model_a3c_new"    # directory where models and validation results are stored, required
    sampler: "epsilon-greedy"                # sampler used for selecting actions, can be greedy, epsilon-greedy, decaying-epsilon-greedy, multinomial
    epsilon: 0.05                            # specific to epsilon-greedy sampler: the probability of sampling randomly from the distribution instead of taking the argmax
    reward_discount_factor: 0.99             # factor for discounting future rewards
    reward_whitening: False                  # if true, normalises the per-agent rewards to have 0 mean and 1 variance
    algorithm: "advantage-actor-critic"      # can be simple-actor-critic, advantage-actor-critic, REINFORCE
    hrl: True                                # if True, scenario is HRL (obsolete)
    multi_agent: True                        # if True, uses the multi-agent version of the RL algorithm specified in algorithm
    bootstrap_from_state_value: False        # specific to advantage-actor-critic: If episode doesn't finish with last state, bootstrap reward from state value of the last observation
    baseline: "value"                        # specific to REINFORCE algorithm: baseline, can be None, average, value
    value_hidden_size: 512                   # hidden dimension of value networks
    value_num_layers: 3                      # number of layers of value networks
    use_target_nets: True                    # specific to advantage-actor-critic: If True, uses target networks together with value networks. If False, uses only value networks
    loss_during_episodes: False              # if True, calculate the loss during episodes instead of after, default: False

testing: # specify which inference algorithm to use for testing (for validation it's always greedy decoding)
    pass: 0
    num_episodes: 3870                       # Number of episodes run for testing
    max_steps_per_episode: 250               # maximum trajectory length per episode during testing
    calculate_accuracy: True                 # If the task supports accuracy scores, they can be calculated by setting this to True

model: # specify your model architecture here
    encoder:
        hidden_size: 512
        rnn_type: "LSTM"
        embedding_dim: 50
        nlayers: 2
        interpolation: 0.5
    num_subtasks: 4
